{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-07T03:43:21.153119Z","iopub.execute_input":"2023-11-07T03:43:21.153605Z","iopub.status.idle":"2023-11-07T03:43:21.163692Z","shell.execute_reply.started":"2023-11-07T03:43:21.153572Z","shell.execute_reply":"2023-11-07T03:43:21.162009Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s3e24/sample_submission.csv\n/kaggle/input/playground-series-s3e24/train.csv\n/kaggle/input/playground-series-s3e24/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntest_data = pd.read_csv('/kaggle/input/playground-series-s3e24/test.csv')\n#print (test_data)\n\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s3e24/train.csv')\n#print (train_data)\n\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cluster import KMeans\nimport xgboost as xgb\nfrom sklearn.ensemble import IsolationForest\n\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n\n\n# Fit the model on all features of the DataFrame\n#iso_forest.fit(test_data)\n\n# Predict if a data point is an outlier\n#outliers = iso_forest.predict(test_data)\n\n# Add a new column for outlier detection\n#test_data['outlier'] = outliers\n\n# Filter the DataFrame to remove outliers\n#test_data_filtered = test_data[test_data['outlier'] == 1].drop('outlier', axis=1)\n\niso_forest.fit(train_data)\n\n# Predict if a data point is an outlier\noutliers = iso_forest.predict(train_data)\n\n# Add a new column for outlier detection\ntrain_data['outlier'] = outliers\n\n# Filter the DataFrame to remove outliers\ntrain_data_filtered = train_data[train_data['outlier'] == 1].drop('outlier', axis=1)\n\n\ntrain_data['BMI'] = train_data['weight(kg)'] / (train_data['height(cm)']/100)**2\ntest_data['BMI'] = test_data['weight(kg)'] / (test_data['height(cm)']/100)**2\n\n\ntrain_data['GH'] = train_data['Gtp'] * (train_data['hemoglobin']**2)\ntest_data['GH'] = test_data['Gtp'] * (test_data['hemoglobin']**2)\n\ntrain_data['GHDL'] = train_data['Gtp'] * (train_data['HDL']**2)\ntest_data['GHDL'] = test_data['Gtp'] * (test_data['HDL']**2)\n\n########Binning#######################\n\n# Define the bins\nbins = [0, 18.5, 24.99, 29.99, 34.99, 39.99, float('inf')]\nlabels = [-1.0, 0.0, 1, 2, 3, 4]\n\n\n# Apply K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=42).fit(train_data)\nkmeansTest = KMeans(n_clusters=3, random_state=42).fit(test_data)\n\n# Create a new feature for the cluster labels\ntrain_data['cluster_label'] = kmeans.labels_\ntest_data['cluster_label'] = kmeansTest.labels_\n\n\nprint ('Cluster labels' )\nprint (train_data['cluster_label'])\n# Bin the BMI column\ntrain_data['BMI_category'] = pd.cut(train_data['BMI'], bins=bins, labels=labels, include_lowest=True)\ntest_data['BMI_category'] = pd.cut(test_data['BMI'], bins=bins, labels=labels, include_lowest=True)\n\n\n\n# Define the bins\nbins = [0, 18.5, 45 , float('inf')]\nlabels = [0, 1, 2]\n\ntrain_data['Age_category'] = pd.cut(train_data['BMI'], bins=bins, labels=labels, include_lowest=True)\ntest_data['Age_category'] = pd.cut(test_data['BMI'], bins=bins, labels=labels, include_lowest=True)\n\n#########Binning#################\n\ntrain_data['Cholesterol Ratio'] = train_data['HDL'] / train_data['LDL']\ntest_data['Cholesterol Ratio'] = test_data['HDL'] / test_data['LDL']\n\ntrain_data['CRA'] = train_data['HDL'] / (train_data['LDL']**2)\ntest_data['CRA'] = test_data['HDL'] / (test_data['LDL']**2)\n\ntrain_data['BMIAP'] = train_data['BMI'] / (train_data['age'] * train_data['age'] )\ntest_data['BMIAP'] = test_data['BMI'] / (test_data['age'] * test_data['age'])\n\ntrain_data['BMIAD'] = (train_data['BMI']**2) / train_data['age'] \ntest_data['BMIAD'] = (test_data['BMI']**2) / test_data['age']\n\ntrain_data['HH'] = train_data['hemoglobin'] * (train_data['height(cm)']**3) \ntest_data['HH'] = test_data['hemoglobin'] * (test_data['height(cm)']**3)\n\ntrain_data['GHH'] = train_data['Gtp'] * (train_data['hemoglobin']**2) \ntest_data['GHH'] = test_data['Gtp'] * (test_data['hemoglobin']**2) \n\ntrain_data['ABR'] = (train_data['AST'] / train_data['BMI']**2 ) * train_data['age']\ntest_data['ABR'] = (test_data['AST'] / test_data['BMI']**2 ) * test_data['age']\n    \ntrain_data['HW_Ratio'] = train_data['height(cm)'] / train_data['waist(cm)']\ntest_data['HW_Ratio'] = test_data['height(cm)'] / test_data['waist(cm)']\n\ntrain_data['HA_Ratio'] = train_data['height(cm)'] / train_data['age']\ntest_data['HA_Ratio'] = test_data['height(cm)'] / test_data['age']\n\ntrain_data['SA'] = train_data['systolic'] / (train_data['age'] **2)\ntest_data['SA'] = test_data['systolic'] / (test_data['age'] **2)\n\n#train_data['LAA'] = train_data['cluster_label'] / (train_data['LDL'] **2)\n#test_data['LAA'] = test_data['cluster_label'] / (test_data['LDL'] **2)\n\ntrain_data['HA'] = train_data['BMI'] / (train_data['height(cm)'] **2)\ntest_data['HA'] = test_data['BMI'] / (test_data['height(cm)'] **2)\n\ntrain_data['CHA'] = train_data['hemoglobin'] / (train_data['systolic'] **2)\ntest_data['CHA'] = test_data['hemoglobin'] / (test_data['systolic'] **2)\n\ntrain_data['LA'] = train_data['LDL'] / (train_data['Gtp'] **3)\ntest_data['LA'] = test_data['LDL'] / (test_data['Gtp'] **3)\n\n\n\ntrain_data['Gtp'] = np.clip(train_data['Gtp'], 0, 300)\ntrain_data['HDL'] = np.clip(train_data['HDL'], 0, 110)\ntrain_data['LDL'] = np.clip(train_data['LDL'], 0, 200)\ntrain_data['ALT'] = np.clip(train_data['ALT'], 0, 150)\ntrain_data['AST'] = np.clip(train_data['AST'], 0, 100)\ntrain_data['serum creatinine'] = np.clip(train_data['serum creatinine'], 0, 3) \n\n\ntest_data['Gtp'] = np.clip(test_data['Gtp'], 0, 300)\ntest_data['HDL'] = np.clip(test_data['HDL'], 0, 110)\ntest_data['LDL'] = np.clip(test_data['LDL'], 0, 200)\ntest_data['ALT'] = np.clip(test_data['ALT'], 0, 150)\ntest_data['AST'] = np.clip(test_data['AST'], 0, 100)\ntest_data['serum creatinine'] = np.clip(test_data['serum creatinine'], 0, 3)\n\n\ncols_to_normalize = ['Cholesterol Ratio','HA_Ratio','cluster_label','SA','HA','HH','BMI','LDL', 'Gtp','AST','ALT' ,'hemoglobin', 'systolic', 'HDL', 'serum creatinine']\n\n            \nscaler = StandardScaler()\ntrain_data[cols_to_normalize] = scaler.fit_transform(train_data[cols_to_normalize])\ntest_data[cols_to_normalize] = scaler.transform(test_data[cols_to_normalize])\n\nx_train, x_test, y_train, y_test = train_test_split(train_data[cols_to_normalize], train_data['smoking'], test_size=0.2, random_state=42)\n\n\n# For each feature apply IQR method\nfor col in cols_to_normalize:\n    Q1 = train_data[col].quantile(0.25)\n    Q3 = train_data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Filter out the outliers from the training data\n    train_data = train_data[(train_data[col] >= lower_bound) & (train_data[col] <= upper_bound)]\n\nfrom sklearn.ensemble import VotingClassifier\n\n# Create the sub-models\nclf1 = KNeighborsClassifier(n_neighbors=45)\nclf2 = xgb.XGBClassifier(n_estimators=1300, learning_rate=.027240097672443227, gamma=0, subsample=0.9772188294492674, colsample_bytree=0.204278500652, max_depth=11)\nclf3 = RandomForestClassifier(n_estimators=1350, max_depth = 150,min_samples_leaf=15,random_state=42,n_jobs=-1, class_weight='balanced')\n\n\n#clf3 = LogisticRegression()\n\n# Create the voting ensemble model (soft voting)\nvoting_clf = VotingClassifier(estimators=[('knn', clf1), ('xgb', clf2), ('lr', clf3)], voting='soft')\n\n# Fit the model on the training data\nvoting_clf.fit(x_train, y_train)\n\n# Predict and evaluate on the test data\npred_probs = voting_clf.predict_proba(x_test)[:, 1]\nauc_score = roc_auc_score(y_test, pred_probs)\nprint(f\"ROC AUC Score Voting: {auc_score}\")\n\n# Get feature importances\n#feature_importances = clf.feature_importances_\n\n# Create a dataframe for visualization\n#features = x_train.columns\n#importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n\n# Sort the dataframe by importance\n#importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Visualize using seaborn's barplot\n#plt.figure(figsize=(12, 8))\n#sns.barplot(x='Importance', y='Feature', data=importance_df)\n#plt.title('Random Forest Feature Importances')\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T03:43:23.604266Z","iopub.execute_input":"2023-11-07T03:43:23.604658Z","iopub.status.idle":"2023-11-07T03:58:27.508050Z","shell.execute_reply.started":"2023-11-07T03:43:23.604628Z","shell.execute_reply":"2023-11-07T03:58:27.506269Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Cluster labels\n0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n159251    0\n159252    0\n159253    0\n159254    0\n159255    0\nName: cluster_label, Length: 159256, dtype: int32\nROC AUC Score Voting: 0.8514095504276453\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Make predictions using the meta_model\ntest_data['prediction'] = voting_clf.predict_proba(test_data[cols_to_normalize])[:, 1]\n\nsubset_data = test_data[['id', 'prediction']]\n\nsubset_data.to_csv(\"/kaggle/working/submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T05:49:05.940473Z","iopub.execute_input":"2023-11-05T05:49:05.940905Z","iopub.status.idle":"2023-11-05T05:53:44.705782Z","shell.execute_reply.started":"2023-11-05T05:49:05.940873Z","shell.execute_reply":"2023-11-05T05:53:44.704746Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#import torch\n#import torch.nn as nn\n#import torch.optim as optim\n#import torch.nn.functional as F\n\n# 2. Prepare Data for PyTorch\n#from torch.utils.data import DataLoader, TensorDataset\n\n# Convert data into PyTorch tensors\n#x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n#y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n#x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n#y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n\n#train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n#test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n\n#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n#test_loader = DataLoader(test_dataset, batch_size=32)\n\n# 3. Define the Neural Network\n#class NeuralNet(nn.Module):\n#    def __init__(self, input_size, dropout_prob=0.5):\n#        super(NeuralNet, self).__init__()\n#        self.fc1 = nn.Linear(input_size, 32)\n#        self.fc2 = nn.Linear(32, 64)\n#        self.fc3 = nn.Linear(64, 128)\n##        self.dropout = nn.Dropout(dropout_prob)\n#        self.fc4 = nn.Linear(128, 2)  # No need for softmax here\n#\n#    def forward(self, x):\n #       x = F.leaky_relu(self.fc1(x))\n#        x = F.leaky_relu(self.fc2(x))\n#        x = F.leaky_relu(self.fc3(x))\n#        x = self.fc4(x)  # Output raw scores\n#        return x\n\n#model = NeuralNet(x_train.shape[1])\n\n# 4. Define Loss Function and Optimizer\n#criterion = nn.CrossEntropyLoss()\n#optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 5. Train the Neural Network\n#num_epochs = 200\n#counter = 0\n#for epoch in range(num_epochs):\n#    print ('Epoch' + str(counter))\n#    counter = counter+ 1\n #   for inputs, labels in train_loader:\n #       outputs = model(inputs)\n #       loss = criterion(outputs, labels.long())\n        \n  #      optimizer.zero_grad()\n  #      loss.backward()\n #      optimizer.step()\n\n# 6. Evaluate the Neural Network\n#model.eval()\n#with torch.no_grad():\n#    correct = 0\n#    total = 0\n#    for inputs, labels in test_loader:\n#        outputs = model(inputs)\n#        _, predicted = torch.max(outputs.data, 1)\n#        total += labels.size(0)\n#        correct += (predicted == labels.long()).sum().item()\n\n#    print('Accuracy: {} %'.format(100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:42:00.420590Z","iopub.execute_input":"2023-11-02T21:42:00.421304Z","iopub.status.idle":"2023-11-02T22:00:42.762017Z","shell.execute_reply.started":"2023-11-02T21:42:00.421249Z","shell.execute_reply":"2023-11-02T22:00:42.760764Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch0\nEpoch1\nEpoch2\nEpoch3\nEpoch4\nEpoch5\nEpoch6\nEpoch7\nEpoch8\nEpoch9\nEpoch10\nEpoch11\nEpoch12\nEpoch13\nEpoch14\nEpoch15\nEpoch16\nEpoch17\nEpoch18\nEpoch19\nEpoch20\nEpoch21\nEpoch22\nEpoch23\nEpoch24\nEpoch25\nEpoch26\nEpoch27\nEpoch28\nEpoch29\nEpoch30\nEpoch31\nEpoch32\nEpoch33\nEpoch34\nEpoch35\nEpoch36\nEpoch37\nEpoch38\nEpoch39\nEpoch40\nEpoch41\nEpoch42\nEpoch43\nEpoch44\nEpoch45\nEpoch46\nEpoch47\nEpoch48\nEpoch49\nEpoch50\nEpoch51\nEpoch52\nEpoch53\nEpoch54\nEpoch55\nEpoch56\nEpoch57\nEpoch58\nEpoch59\nEpoch60\nEpoch61\nEpoch62\nEpoch63\nEpoch64\nEpoch65\nEpoch66\nEpoch67\nEpoch68\nEpoch69\nEpoch70\nEpoch71\nEpoch72\nEpoch73\nEpoch74\nEpoch75\nEpoch76\nEpoch77\nEpoch78\nEpoch79\nEpoch80\nEpoch81\nEpoch82\nEpoch83\nEpoch84\nEpoch85\nEpoch86\nEpoch87\nEpoch88\nEpoch89\nEpoch90\nEpoch91\nEpoch92\nEpoch93\nEpoch94\nEpoch95\nEpoch96\nEpoch97\nEpoch98\nEpoch99\nEpoch100\nEpoch101\nEpoch102\nEpoch103\nEpoch104\nEpoch105\nEpoch106\nEpoch107\nEpoch108\nEpoch109\nEpoch110\nEpoch111\nEpoch112\nEpoch113\nEpoch114\nEpoch115\nEpoch116\nEpoch117\nEpoch118\nEpoch119\nEpoch120\nEpoch121\nEpoch122\nEpoch123\nEpoch124\nEpoch125\nEpoch126\nEpoch127\nEpoch128\nEpoch129\nEpoch130\nEpoch131\nEpoch132\nEpoch133\nEpoch134\nEpoch135\nEpoch136\nEpoch137\nEpoch138\nEpoch139\nEpoch140\nEpoch141\nEpoch142\nEpoch143\nEpoch144\nEpoch145\nEpoch146\nEpoch147\nEpoch148\nEpoch149\nEpoch150\nEpoch151\nEpoch152\nEpoch153\nEpoch154\nEpoch155\nEpoch156\nEpoch157\nEpoch158\nEpoch159\nEpoch160\nEpoch161\nEpoch162\nEpoch163\nEpoch164\nEpoch165\nEpoch166\nEpoch167\nEpoch168\nEpoch169\nEpoch170\nEpoch171\nEpoch172\nEpoch173\nEpoch174\nEpoch175\nEpoch176\nEpoch177\nEpoch178\nEpoch179\nEpoch180\nEpoch181\nEpoch182\nEpoch183\nEpoch184\nEpoch185\nEpoch186\nEpoch187\nEpoch188\nEpoch189\nEpoch190\nEpoch191\nEpoch192\nEpoch193\nEpoch194\nEpoch195\nEpoch196\nEpoch197\nEpoch198\nEpoch199\nAccuracy: 75.05337184478212 %\n","output_type":"stream"}]}]}